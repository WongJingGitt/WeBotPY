import os
import json
import time
from typing import TypedDict, List, Dict, Any, Optional

from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langgraph.graph import StateGraph, END

from llm.llm import LLMFactory

# --- 1. å®šä¹‰çŠ¶æ€ï¼ˆç±»å¤–éƒ¨ï¼‰ ---
class AgentState(TypedDict):
    input_dict: Dict[str, Any]      # åŸå§‹èŠå¤©è®°å½•å­—å…¸
    user_query: str                 # ç”¨æˆ·çš„åŸå§‹é—®é¢˜
    # --- æŸ¥è¯¢ç†è§£ ---
    intent: Optional[str]           # æ¨æ–­çš„ç”¨æˆ·æ„å›¾
    entities: Optional[Dict]        # æå–çš„å…³é”®å®ä½“
    chunk_processing_prompt: Optional[str] # åŠ¨æ€ç”Ÿæˆçš„ç”¨äºå¤„ç†å—çš„ Prompt
    # --- åˆ†å— ---
    messages: List[Dict]            # ä» input_dict æå–çš„åŸå§‹æ¶ˆæ¯åˆ—è¡¨
    message_chunks: List[List[Dict]]# åˆ†å—åçš„æ¶ˆæ¯åˆ—è¡¨
    # --- æå– ---
    extracted_data: List[str]       # ä»å„å—æå–çš„ä¿¡æ¯åˆ—è¡¨
    # --- æœ€ç»ˆç­”æ¡ˆ ---
    final_answer: Optional[str]     # æœ€ç»ˆç»™ç”¨æˆ·çš„ç­”æ¡ˆ
    # --- é”™è¯¯å¤„ç† ---
    error_message: Optional[str]    # è®°å½•å¤„ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯

# --- 2. å®šä¹‰Agentç±» ---
class ChatSplitterAgent:
    """
    ä¸€ä¸ªä½¿ç”¨ LangGraph æ„å»ºçš„ Agentï¼Œç”¨äºåˆ†æé•¿èŠå¤©è®°å½•å¹¶å›ç­”ç‰¹å®šé—®é¢˜ã€‚
    å®ƒä½¿ç”¨å­—èŠ‚æ•°æ¥æ§åˆ¶æ–‡æœ¬åˆ†å—ï¼Œä»¥é€‚åº”åŸºäºå­—èŠ‚æ•°è®¡è´¹/é™åˆ¶çš„æ¨¡å‹ã€‚
    """
    def __init__(
        self,
        llm_query_understanding: BaseChatModel, # ç”¨äºç†è§£æŸ¥è¯¢å’Œè§„åˆ’çš„ LLM å®ä¾‹ã€‚
        llm_extraction: BaseChatModel = None,  # ç”¨äºä»å—ä¸­æå–ä¿¡æ¯çš„ LLM å®ä¾‹ã€‚
        llm_synthesis: BaseChatModel = None,   # ç”¨äºåˆæˆæœ€ç»ˆç­”æ¡ˆçš„ LLM å®ä¾‹ã€‚
        max_bytes_per_chunk: int = 12000, # åŸºäºå­—èŠ‚æ•°çš„å—å¤§å°ä¸Šé™ (éœ€è¦æ ¹æ®æ¨¡å‹è°ƒæ•´)
        prompt_overhead_bytes: int = 500,  # ä¼°ç®—çš„ Prompt å¼€é”€å­—èŠ‚æ•° (éœ€è¦è°ƒæ•´)
        byte_encoding: str = 'utf-8',      # ç”¨äºè®¡ç®—å­—èŠ‚æ•°çš„ç¼–ç 
        recursion_limit: int = 15,
        rpm_limit = 10,

    ):
        """
        åˆå§‹åŒ– Agent.

        Args:
            llm_query_understanding: ç”¨äºç†è§£æŸ¥è¯¢å’Œè§„åˆ’çš„ LLM å®ä¾‹ã€‚
            llm_extraction: ç”¨äºä»å—ä¸­æå–ä¿¡æ¯çš„ LLM å®ä¾‹ã€‚
            llm_synthesis: ç”¨äºæœ€ç»ˆåˆæˆç­”æ¡ˆçš„ LLM å®ä¾‹ã€‚
            max_bytes_per_chunk: æ¯ä¸ªå—çš„æœ€å¤§ç›®æ ‡å­—èŠ‚æ•° (ä¸å« Prompt å¼€é”€)ã€‚
            prompt_overhead_bytes: ä¸º Prompt å’Œå…¶ä»–å¼€é”€é¢„ç•™çš„ä¼°è®¡å­—èŠ‚æ•°ã€‚
            byte_encoding: è®¡ç®—å­—èŠ‚æ•°æ—¶ä½¿ç”¨çš„å­—ç¬¦ä¸²ç¼–ç ã€‚
            recursion_limit: LangGraph çš„é€’å½’æ·±åº¦é™åˆ¶ã€‚
            rpm_limit: æ¨¡å‹æ¯åˆ†é’Ÿå¤„ç†çš„æœ€å¤§è¯·æ±‚æ•°ã€‚
        """
        # è®¾ç½® LLM å®ä¾‹ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if not isinstance(llm_query_understanding, BaseChatModel):
            raise ValueError("llm_query_understanding must be a BaseChatModel instance.")
        
        self.llm_query_understanding = llm_query_understanding 
        self.llm_extraction = llm_extraction or llm_query_understanding
        self.llm_synthesis = llm_synthesis or llm_query_understanding

        # é…ç½®å‚æ•°
        self.max_bytes_per_chunk = max_bytes_per_chunk
        self.prompt_overhead_bytes = prompt_overhead_bytes
        self.byte_encoding = byte_encoding
        self.recursion_limit = recursion_limit
        self.rpm_limit = rpm_limit

        # æ„å»ºå¹¶ç¼–è¯‘ LangGraph åº”ç”¨
        self.app = self._build_graph()

    # --- è¾…åŠ©æ–¹æ³• ---
    def _format_single_message_for_llm(self, message: Dict) -> str:
        """å°†å•æ¡æ¶ˆæ¯å­—å…¸æ ¼å¼åŒ–ä¸ºç®€æ´çš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚"""
        sender = message.get('sender', 'Unknown')
        remark = message.get('remark')
        content = message.get('content', '')
        timestamp = message.get('time', '')
        prefix = f"{sender}"
        if remark:
            prefix += f" ({remark})"

        # ç®€åŒ–ç‰¹æ®Šæ¶ˆæ¯è¡¨ç¤º
        if content.startswith('[') and ']' in content:
            try:
                main_type_end = content.find(':') if ':' in content else content.find(']')
                if main_type_end != -1:
                     main_type = content[1:main_type_end]
                     content = f"[{main_type}æ¶ˆæ¯]"
                else:
                     content = "[ç‰¹æ®Šæ¶ˆæ¯]"
            except:
                content = "[ç‰¹æ®Šæ¶ˆæ¯]" # Fallback

        return f"{timestamp} - {prefix}: {content}"

    def _format_chunk_for_llm(self, chunk: List[Dict[str, Any]]) -> str:
        """å°†æ¶ˆæ¯å­—å…¸åˆ—è¡¨ï¼ˆä¸€ä¸ªå—ï¼‰è½¬æ¢ä¸ºå¤šè¡Œæ–‡æœ¬è¡¨ç¤ºã€‚"""
        return "\n".join([self._format_single_message_for_llm(msg) for msg in chunk])

    def _chunk_by_byte_count(self, messages: List[Dict]) -> List[List[Dict]]:
        """æŒ‰å­—èŠ‚æ•°åˆ†å‰²æ¶ˆæ¯åˆ—è¡¨ã€‚"""
        chunks = []
        current_chunk = []
        current_byte_count = 0
        effective_max_bytes = self.max_bytes_per_chunk - self.prompt_overhead_bytes
        if effective_max_bytes <= 0:
             raise ValueError("max_bytes_per_chunk is too small compared to prompt_overhead_bytes.")

        print("\n",f"   å¼€å§‹æŒ‰å­—èŠ‚æ•°åˆ†å—ï¼ˆæ¯å—æœ‰æ•ˆæœ€å¤§å­—èŠ‚æ•°ï¼š{effective_max_bytes}ï¼‰...")

        for message in messages:
            formatted_message = self._format_single_message_for_llm(message)
            try:
                message_bytes = len(formatted_message.encode(self.byte_encoding))
            except Exception as e:
                print("\n",f"è­¦å‘Šï¼šæ— æ³•ç¼–ç æ¶ˆæ¯ï¼Œè·³è¿‡å…¶å­—èŠ‚è®¡æ•°ã€‚é”™è¯¯ï¼š{e}")
                message_bytes = 0 # æˆ–è€…ç»™ä¸€ä¸ªä¼°è®¡å€¼

            # æ£€æŸ¥å•æ¡æ¶ˆæ¯æ˜¯å¦è¶…é™
            if message_bytes > effective_max_bytes:
                print("\n",f"è­¦å‘Šï¼šå•æ¡æ¶ˆæ¯è¶…è¿‡æœ‰æ•ˆæœ€å¤§å­—èŠ‚é™åˆ¶ï¼ˆ{message_bytes} > {effective_max_bytes}ï¼‰ã€‚è·³è¿‡è¿™æ¡æ¶ˆæ¯ï¼š{formatted_message[:100]}...")
                continue # è·³è¿‡è¿™æ¡è¿‡é•¿çš„æ¶ˆæ¯

            # æ£€æŸ¥åŠ å…¥è¿™æ¡æ¶ˆæ¯åæ˜¯å¦ä¼šè¶…é™
            if current_byte_count + message_bytes > effective_max_bytes and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                chunks.append(current_chunk)
                current_chunk = [message]
                current_byte_count = message_bytes
            else:
                # åŠ å…¥å½“å‰å—
                current_chunk.append(message)
                current_byte_count += message_bytes

        # åŠ å…¥æœ€åä¸€ä¸ªå—ï¼ˆå¦‚æœéç©ºï¼‰
        if current_chunk:
            chunks.append(current_chunk)

        print("\n",f"   åˆ†å—å®Œæˆï¼š{len(messages)} æ¡æ¶ˆæ¯ -> {len(chunks)} ä¸ªå—ï¼ˆç›®æ ‡æœ€å¤§å­—èŠ‚æ•°ï¼š{self.max_bytes_per_chunk}ï¼‰")
        return chunks

    # --- å›¾èŠ‚ç‚¹æ–¹æ³• ---
    def _understand_query_node(self, state: AgentState) -> Dict[str, Any]:
        """èŠ‚ç‚¹ï¼šç†è§£æŸ¥è¯¢ä¸è§„åˆ’ã€‚"""
        print("\n","--- è¿è¡ŒèŠ‚ç‚¹ï¼šunderstand_query_node ---")
        user_query = state['user_query']
        context = state['input_dict'].get('meta', {'context': []}).get('context')
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä½æ™ºèƒ½ä»»åŠ¡è§„åˆ’å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·å…³äºèŠå¤©è®°å½•çš„é—®é¢˜ï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„æŒ‡ä»¤ï¼ˆPromptï¼‰ï¼Œç”¨äºæŒ‡å¯¼åç»­æ­¥éª¤ä»èŠå¤©è®°å½•çš„ *å•ä¸ª* æ–‡æœ¬å—ä¸­æå–æ‰€éœ€ä¿¡æ¯ã€‚

    è¯·è¯†åˆ«ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾å’Œå…³é”®å®ä½“ã€‚ç„¶åï¼Œæ ¹æ®æ„å›¾å’Œå®ä½“ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€æ˜ç¡®ã€å¯æ“ä½œçš„ Promptï¼Œè¿™ä¸ª Prompt å°†è¢«åº”ç”¨äºèŠå¤©è®°å½•çš„æ¯ä¸ªå°å—æ–‡æœ¬ã€‚

    åœ¨ç”ŸæˆæŒ‡ä»¤æ—¶ï¼Œä½ å¯ä»¥å‚è€ƒç”±AIç”Ÿæˆçš„å†å²èŠå¤©ä¸Šä¸‹æ–‡ååŠ©ä½ ç”ŸæˆæŒ‡ä»¤ã€‚ä¾‹å¦‚ï¼šç‰¹å®šçš„æ¢—ã€æ˜µç§°ã€äº‹ä»¶ã€è¡Œä¸ºç­‰ç­‰
             
    è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
    - "intent": å¯¹ç”¨æˆ·æ„å›¾çš„ç®€çŸ­æè¿° (ä¾‹å¦‚: "æ€§æ ¼åˆ†æ", "äº‹ä»¶æ€»ç»“", "æŸ¥æ‰¾ç‰¹å®šå‘è¨€", "å¸¸è§„æ‘˜è¦")ã€‚
    - "entities": ä¸€ä¸ªåŒ…å«å…³é”®å®ä½“çš„å­—å…¸ (ä¾‹å¦‚: {{"person": "å¼ ä¸‰"}}, {{"date": "2025-04-08"}}, {{"topic": "é¡¹ç›®ä¼šè®®"}})ã€‚å¦‚æœæ— æ˜æ˜¾å®ä½“ï¼Œåˆ™ä¸ºç©ºå­—å…¸ã€‚
    - "chunk_processing_prompt": ç”Ÿæˆçš„ç”¨äºå¤„ç†å•ä¸ªæ–‡æœ¬å—çš„ Prompt å­—ç¬¦ä¸²ã€‚è¿™ä¸ª Prompt åº”è¯¥æŒ‡å¯¼å¦‚ä½•ä»ä¸€å°æ®µèŠå¤©è®°å½•ä¸­æå–ä¸ç”¨æˆ·åŸå§‹é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·é—®"å¼ ä¸‰çš„æ€§æ ¼"ï¼Œè¿™ä¸ª Prompt åº”è¯¥è¦æ±‚æå–"å¼ ä¸‰"åœ¨è¯¥å—ä¸­çš„å‘è¨€ã€‚

    ç”¨æˆ·é—®é¢˜:
    {user_query}
    
    å†å²èŠå¤©ä¸Šä¸‹æ–‡:
    {context}

    è¯·ç”Ÿæˆ JSON è¾“å‡ºï¼š"""),
            ("human", "{user_query}") # å†æ¬¡æä¾› user_query å¯èƒ½æœ‰åŠ©äºæŸäº›æ¨¡å‹
        ])
        parser = JsonOutputParser()
        chain = prompt_template | self.llm_query_understanding | parser
        try:
            print("\n",f"   åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼š'{user_query}'")
            response = chain.invoke({"user_query": user_query, "context": context})
            print("\n",f"   LLMåˆ†æç»“æœï¼š{response}")
            if not all(k in response for k in ["intent", "entities", "chunk_processing_prompt"]) or not response.get("chunk_processing_prompt"):
                 raise ValueError("LLMå¯¹æŸ¥è¯¢ç†è§£çš„å“åº”æ— æ•ˆã€‚")
            return {
                "intent": response.get("intent"),
                "entities": response.get("entities"),
                "chunk_processing_prompt": response.get("chunk_processing_prompt")
            }
        except Exception as e:
            print("\n",f"   understand_query_nodeä¸­å‡ºé”™ï¼š{e}")
            return {"error_message": f"æ— æ³•ç†è§£æŸ¥è¯¢æˆ–ç”Ÿæˆå¤„ç†æç¤ºï¼š{e}"}

    def _chunk_node(self, state: AgentState) -> Dict[str, Any]:
        """èŠ‚ç‚¹ï¼šåŠ è½½æ¶ˆæ¯å¹¶æŒ‰å­—èŠ‚æ•°åˆ†å—ã€‚"""
        print("\n",f"--- è¿è¡ŒèŠ‚ç‚¹ï¼šchunk_nodeï¼ˆæœ€å¤§å­—èŠ‚æ•°ï¼š{self.max_bytes_per_chunk}ï¼‰---")
        if state.get("error_message"): return {}
        try:
            messages = state['input_dict'].get('data', [])
            if not messages:
                return {"error_message": "è¾“å…¥æ•°æ®ä¸­æœªæ‰¾åˆ°æ¶ˆæ¯ã€‚"}
            message_chunks = self._chunk_by_byte_count(messages)
            if not message_chunks:
                 return {"error_message": "åˆ†å—ç»“æœä¸ºé›¶å—ã€‚è¯·æ£€æŸ¥æ•°æ®æˆ–åˆ†å—é€»è¾‘ã€‚"}
            return {"messages": messages, "message_chunks": message_chunks}
        except Exception as e:
            print("\n",f"   chunk_nodeä¸­å‡ºé”™ï¼š{e}")
            return {"error_message": f"æ¶ˆæ¯åˆ†å—è¿‡ç¨‹ä¸­å¤±è´¥ï¼š{e}"}

    def _extract_info_node(self, state: AgentState) -> Dict[str, Any]:
        """èŠ‚ç‚¹ï¼šåˆ†å—ä¿¡æ¯æå–ã€‚"""
        print("\n","--- è¿è¡ŒèŠ‚ç‚¹ï¼šextract_info_node ---")
        if state.get("error_message"): return {}
        message_chunks = state.get('message_chunks')
        chunk_processing_prompt = state.get('chunk_processing_prompt')
        if not message_chunks or not chunk_processing_prompt:
            return {"error_message": "ç¼ºå°‘æ¶ˆæ¯å—æˆ–æå–çš„å¤„ç†æç¤ºã€‚"}

        extracted_data = []
        parser = StrOutputParser()
        prompt_template = ChatPromptTemplate.from_template(
            f"{chunk_processing_prompt}\n\nèŠå¤©è®°å½•ç‰‡æ®µ:\n```\n{{chunk_text}}\n```\n\næå–çš„ç›¸å…³ä¿¡æ¯ (å¦‚æœæ­¤ç‰‡æ®µä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜'æ— ç›¸å…³ä¿¡æ¯'):"
        )
        chain = prompt_template | self.llm_extraction | parser
        print("\n",f"   ä½¿ç”¨ç”Ÿæˆçš„æç¤ºå¤„ç†{len(message_chunks)}ä¸ªå—...")

        min_interval = 60.0 / self.rpm_limit if self.rpm_limit > 0 else 0
        last_call_time = time.monotonic()
        for i, chunk in enumerate(message_chunks):
            formatted_chunk = self._format_chunk_for_llm(chunk)
            if not formatted_chunk.strip():
                print("\n",f"   è·³è¿‡ç©ºå— {i+1}/{len(message_chunks)}")
                continue
            try:
                current_time = time.monotonic()
                elapsed = current_time - last_call_time
                if elapsed < min_interval:
                    wait_time = min_interval - elapsed
                    print("\n",f"   ç­‰å¾…{wait_time:.2f}ç§’ä»¥é¿å…é€Ÿç‡é™åˆ¶...")
                    time.sleep(wait_time)

                result = chain.invoke({"chunk_text": formatted_chunk})
                last_call_time = time.monotonic()
                if "æ— ç›¸å…³ä¿¡æ¯" not in result: # è¿‡æ»¤æ‰æ˜ç¡®çš„å¦å®šå›ç­”
                    extracted_data.append(result)
            except Exception as e:
                print("\n",f"   å¤„ç†å—{i+1}æ—¶å‡ºé”™ï¼š{e}")
                last_call_time = time.monotonic()
                extracted_data.append(f"[å¤„ç†å—{i+1}æ—¶å‡ºé”™ï¼š{e}]")
            print("\n",f"   å·²å¤„ç†å— {i+1}/{len(message_chunks)}")
        print("\n",f"   æå–å®Œæˆã€‚åœ¨{len(extracted_data)}ä¸ªå—ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
        return {"extracted_data": extracted_data}

    def _synthesize_answer_node(self, state: AgentState) -> Dict[str, Any]:
        """èŠ‚ç‚¹ï¼šæœ€ç»ˆåˆæˆç­”æ¡ˆã€‚"""
        print("\n","--- è¿è¡ŒèŠ‚ç‚¹ï¼šsynthesize_answer_node ---")
        if state.get("error_message"): return {}
        user_query = state['user_query']
        extracted_data = state.get('extracted_data')
        intent = state.get('intent', 'å›ç­”ç”¨æˆ·é—®é¢˜')
        if not extracted_data:
            print("\n","   æœªæå–åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
            return {"final_answer": f"æ ¹æ®æä¾›çš„èŠå¤©è®°å½•ï¼Œæœªèƒ½æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ '{user_query}' ç›´æ¥ç›¸å…³çš„ä¿¡æ¯ã€‚"}
        if not user_query:
             return {"error_message": "æœ€ç»ˆåˆæˆç¼ºå°‘ç”¨æˆ·æŸ¥è¯¢ã€‚"}

        combined_context = "\n\n---\n\n".join(extracted_data)
        prompt_template = ChatPromptTemplate.from_template(
             f"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ˜¯ï¼š\"{user_query}\"ã€‚\n"
             f"æ ¹æ®ä»é•¿èŠå¤©è®°å½•ä¸­æå–çš„ç›¸å…³ä¿¡æ¯ç‰‡æ®µï¼Œè¯·ç»¼åˆåˆ†æå¹¶å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ã€‚\n"
             f"ç”¨æˆ·çš„æ„å›¾æ˜¯ï¼š{intent}ã€‚\n\n"
             "æå–çš„ç›¸å…³ä¿¡æ¯ç‰‡æ®µå¦‚ä¸‹:\n"
             "```\n{combined_context}\n```\n\n"
             "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œæ¸…æ™°ã€è¿è´¯åœ°å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ï¼š\"{user_query}\"\n"
             "æœ€ç»ˆå›ç­”:"
        )
        
        parser = StrOutputParser()
        chain = prompt_template | self.llm_synthesis | parser
        try:
            print("\n","\n","   åˆæˆæœ€ç»ˆç­”æ¡ˆ...")
            final_answer = chain.invoke({"combined_context": combined_context, "user_query": user_query})
            print("\n","   å·²ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚")
            return {"final_answer": final_answer}
        except Exception as e:
            print("\n",f"   synthesize_answer_nodeä¸­å‡ºé”™ï¼š{e}")
            return {"error_message": f"æœ€ç»ˆç­”æ¡ˆåˆæˆè¿‡ç¨‹ä¸­å¤±è´¥ï¼š{e}"}

    def _handle_error_node(self, state: AgentState) -> Dict[str, Any]:
        """èŠ‚ç‚¹ï¼šå¤„ç†é”™è¯¯ã€‚"""
        print("\n","--- è¿è¡ŒèŠ‚ç‚¹ï¼šhandle_error_node ---")
        error = state.get("error_message", "å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚")
        print("\n",f"   æ•è·åˆ°é”™è¯¯ï¼š{error}")
        return {"final_answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ï¼š\n{error}"}

    # --- æ¡ä»¶è¾¹ç¼˜é€»è¾‘ ---
    def _should_continue(self, state: AgentState) -> str:
        """å†³å®šæ˜¯ç»§ç»­è¿˜æ˜¯è·³è½¬åˆ°é”™è¯¯å¤„ç†ã€‚"""
        if state.get("error_message"):
            print("\n","--- è¾¹ç¼˜æ¡ä»¶ï¼šæ£€æµ‹åˆ°é”™è¯¯ï¼Œè·¯ç”±åˆ°handle_error ---")
            return "error"
        else:
            # print("\n","--- è¾¹ç¼˜æ¡ä»¶ï¼šæ— é”™è¯¯ï¼Œç»§ç»­æ­£å¸¸æµç¨‹ ---") # å‡å°‘æ‰“å°
            return "continue"

    # --- å›¾æ„å»ºæ–¹æ³• ---
    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµã€‚"""
        workflow = StateGraph(AgentState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("understand_query", self._understand_query_node)
        workflow.add_node("chunker", self._chunk_node)
        workflow.add_node("extract_info", self._extract_info_node)
        workflow.add_node("synthesize_answer", self._synthesize_answer_node)
        workflow.add_node("handle_error", self._handle_error_node)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("understand_query")

        # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges("understand_query", self._should_continue, {"continue": "chunker", "error": "handle_error"})
        workflow.add_conditional_edges("chunker", self._should_continue, {"continue": "extract_info", "error": "handle_error"})
        workflow.add_conditional_edges("extract_info", self._should_continue, {"continue": "synthesize_answer", "error": "handle_error"}) # å³ä½¿æå–æœ‰é”™ä¹Ÿå°è¯•åˆæˆ
        workflow.add_conditional_edges("synthesize_answer", self._should_continue, {"continue": END, "error": "handle_error"})
        workflow.add_edge("handle_error", END)

        # ç¼–è¯‘å›¾
        print("\n","Agentå›¾æ„å»ºæˆåŠŸã€‚")
        return workflow.compile()

    # --- å…¬å…±æ‰§è¡Œæ–¹æ³• ---
    def run(self, chat_data: Dict[str, Any], user_query: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œ Agent æ¥å¤„ç†èŠå¤©æ•°æ®å¹¶å›ç­”é—®é¢˜ã€‚

        Args:
            chat_data: åŒ…å« 'meta' å’Œ 'data' çš„èŠå¤©è®°å½•å­—å…¸ã€‚
            user_query: ç”¨æˆ·çš„é—®é¢˜å­—ç¬¦ä¸²ã€‚

        Returns:
            åŒ…å«æœ€ç»ˆçŠ¶æ€çš„å­—å…¸ï¼Œå…¶ä¸­ 'final_answer' æ˜¯ç»™ç”¨æˆ·çš„ç­”æ¡ˆã€‚
        """
        if not isinstance(chat_data, dict) or 'data' not in chat_data:
            raise ValueError("Invalid chat_data format. Expected a dict with a 'data' key.")
        if not isinstance(user_query, str) or not user_query:
             raise ValueError("user_query must be a non-empty string.")

        initial_state = {
            "input_dict": chat_data,
            "user_query": user_query,
            # åˆå§‹åŒ–å…¶ä»–å­—æ®µä¸º None æˆ–ç©ºåˆ—è¡¨/å­—å…¸
            "intent": None,
            "entities": None,
            "chunk_processing_prompt": None,
            "messages": [],
            "message_chunks": [],
            "extracted_data": [],
            "final_answer": None,
            "error_message": None,
        }

        print("\n","\n--- å¼€å§‹Agentæ‰§è¡Œ ---")
        # ä½¿ç”¨ invoke è·å–æœ€ç»ˆç»“æœ
        final_state = self.app.invoke(initial_state, config={"recursion_limit": self.recursion_limit})
        print("\n","--- Agentæ‰§è¡Œå®Œæˆ ---")

        return final_state

# TODO: 
#   1. å¢åŠ ä»»åŠ¡è¡¨æ ¼ç»‘å®šæ‹“å±•æ–­ç‚¹é‡è¯•
#   2. ä½¿ç”¨ Celery æˆ– RQåˆ›å»ºä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ `pip install celery redis`
#   3. æœ€åèåˆæ€»ç»“æ—¶ï¼Œè‹¥æ˜¯å¾—å‡ºçš„chunkæ€»ç»“åˆè¶…å‡ºäº†èåˆæ¨¡å‹çš„æœ€å¤§è¾“å…¥ã€‚åˆè¦åˆ†å—ï¼Ÿ
#       CREATE TABLE IF NOT EXISTS long_tasks (
#             task_id TEXT PRIMARY KEY,              -- ä»»åŠ¡å”¯ä¸€ID (å»ºè®®ä½¿ç”¨ UUID)
#             conversation_id TEXT NOT NULL,         -- å¯¹åº”å‰ç«¯çš„å¯¹è¯ ID
#             triggering_message_id TEXT,            -- è§¦å‘æ­¤ä»»åŠ¡çš„ç”¨æˆ·æ¶ˆæ¯ ID (å¯é€‰)
#             user_query TEXT NOT NULL,              -- ç”¨æˆ·çš„åŸå§‹è¯·æ±‚
#             input_data_ref TEXT,                   -- æŒ‡å‘è¾“å…¥æ•°æ®çš„æ–¹å¼ (ä¾‹å¦‚: æ–‡ä»¶è·¯å¾„, S3 key, æˆ–ç›´æ¥å­˜å‚¨å°è¾“å…¥çš„ JSON)
#             status TEXT NOT NULL CHECK(status IN ('PENDING', 'PLANNING', 'CHUNKING', 'EXTRACTING', 'SYNTHESIZING', 'COMPLETED', 'FAILED', 'PAUSED')), -- ä»»åŠ¡çŠ¶æ€
#             current_step TEXT,                     -- å½“å‰æˆ–æœ€åæˆåŠŸå®Œæˆçš„ LangGraph èŠ‚ç‚¹å (å¯é€‰)
#             total_chunks INTEGER,                  -- æ€»åˆ†å—æ•° (å¯é€‰, ç”¨äºè¿›åº¦æ˜¾ç¤º)
#             processed_chunk_index INTEGER DEFAULT -1, -- æœ€åæˆåŠŸå¤„ç†çš„å—çš„ç´¢å¼• (ä»0å¼€å§‹, -1è¡¨ç¤ºè¿˜æœªå¼€å§‹)
#             intermediate_results TEXT,             -- å­˜å‚¨ç´¯ç§¯çš„æå–ç»“æœ (ä¾‹å¦‚: JSON æ ¼å¼çš„åˆ—è¡¨)
#             final_answer TEXT,                     -- æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
#             error_message TEXT,                    -- è®°å½•é”™è¯¯ä¿¡æ¯
#             retry_count INTEGER DEFAULT 0,         -- é‡è¯•æ¬¡æ•°
#             created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
#             updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
#         );
#         -- å¯ä»¥ä¸º conversation_id åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
#         CREATE INDEX IF NOT EXISTS idx_conversation_id ON long_tasks (conversation_id);
#   4. è€ƒè™‘æŠŠintermediate_resultsæŒ‡å‘åˆ°JSONLä¸­ï¼Œä¼˜åŒ–SQLiteçš„æ€§èƒ½


if __name__ == "__main__":
    
    with open('D:\wangyingjie\WeBot\data\exports\ä¸Šæµ·äº¤å¤§ğŸ‡¨ğŸ‡³äººç”Ÿä½•å¤„ä¸é’å±±__2025-04-02_17-55-22.txt', 'r', encoding='utf-8') as r:
        chat_data = json.load(r)
    
    user_query = "è¯·ä½ é’ˆå¯¹è¿™ä»½èŠå¤©è®°å½•æ·±åº¦åˆ†æä¸€ä¸‹åˆ˜å¥¶å’Œæé˜³ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”åˆ—å‡ºä¸€äº›æ•°æ®æ¥æ”¯æ’‘çš„ä½ ç»“è®º"

    try:
        agent = ChatSplitterAgent(
            max_bytes_per_chunk=20480,
            prompt_overhead_bytes=2048,
            llm_query_understanding=LLMFactory.deepseek_v3_llm(),
            llm_extraction=LLMFactory.gemini_llm(),
            llm_synthesis=LLMFactory.deepseek_v3_llm()
        )

        # è¿è¡Œ Agent
        final_state = agent.run(chat_data, user_query)

        # æ‰“å°æœ€ç»ˆç­”æ¡ˆ
        print("\n","\n--- æœ€ç»ˆç­”æ¡ˆ ---")
        print("\n",final_state.get("final_answer", "æœªç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"))
        if final_state.get("error_message"):
             print("\n",f"\n--- è®°å½•çš„é”™è¯¯ä¿¡æ¯ ---")
             print("\n",final_state.get("error_message"))

    except ValueError as ve:
         print("\n",f"è¾“å…¥é”™è¯¯ï¼š{ve}")
    except Exception as e:
         print("\n",f"Agentæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")